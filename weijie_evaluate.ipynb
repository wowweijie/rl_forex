{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from finrl.config import config\n",
    "from finrl.preprocessing.preprocessors import FeatureEngineer\n",
    "from finrl.preprocessing.data import data_split\n",
    "from finrl.env.env_stocktrading import StockTradingEnv\n",
    "from finrl.model.models import DRLAgent\n",
    "from finrl.trade.backtest import BackTestStats, BaselineStats, BackTestPlot\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl.preprocessing import data, preprocessors\n",
    "EURUSD_df=data.load_ohlc_dataset(\"15min/EURUSD/01_17.csv\")\n",
    "GBPUSD_df=data.load_ohlc_dataset(\"15min/GBPUSD/01_17.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_map = {\n",
    "                'sma_9': {'talib_name' : 'SMA', 'time_period' : 9}, #time_period in seoonds\n",
    "                'ema_9': {'talib_name' : 'EMA', 'time_period' : 9}, #time_period in seoonds\n",
    "                'sma_21' : {'talib_name' : 'SMA', 'time_period' : 21},\n",
    "                'ema_21' : {'talib_name' : 'EMA', 'time_period' : 21},\n",
    "                'bbands_9':{'talib_name':'BBANDS','time_period':9,'nbdevup':2.0,'nbdevdn':2.0},\n",
    "                'bbands_12':{'talib_name':'BBANDS','time_period':12,'nbdevup':2.0,'nbdevdn':2.0},\n",
    "                'macd_entry':{'talib_name':'MACD', 'fastperiod':12, 'slowperiod':26,'signalperiod':9},\n",
    "                'macd_exit':{'talib_name':'MACD', 'fastperiod':19, 'slowperiod':39,'signalperiod':9},\n",
    "                'stoch':{'talib_name':'STOCH', 'fastk_period':5, 'slowk_period':3, 'slowk_matype':0, 'slowd_period':3, 'slowd_matype':0},\n",
    "                'rsi_14':{'talib_name':'RSI', 'time_period':14},\n",
    "                'rsi_4':{'talib_name':'RSI','time_period':4},\n",
    "                'mom_10':{'talib_name':'MOM', 'time_period':10},\n",
    "                'stochrsi_14':{'talib_name':'STOCHRSI', 'time_period':14, 'fastk_period':5,'fastd_period':3, 'fastd_matype':0},\n",
    "                'kama_30':{'talib_name':'KAMA', 'time_period':30},\n",
    "                't3_5':{'talib_name':'T3', 'time_period':5, 'vfactor':0.7},\n",
    "                'atr_14':{'talib_name':'ATR', 'time_period':14},\n",
    "                'natr_14':{'talib_name':'NATR', 'time_period':14},\n",
    "                'tsf_14':{'talib_name':'TSF', 'time_period':14},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl.preprocessing import data, preprocessors\n",
    "\n",
    "EURUSD_train, tech_indicator_list = preprocessors.FeatureEngineer(EURUSD_df,\n",
    "                          tech_indicator_params_map = param_map,\n",
    "                          use_technical_indicator=True,\n",
    "                          user_defined_feature=False).preprocess_data()\n",
    "\n",
    "GBPUSD_train, tech_indicator_list = preprocessors.FeatureEngineer(GBPUSD_df,\n",
    "                          tech_indicator_params_map = param_map,\n",
    "                          use_technical_indicator=True,\n",
    "                          user_defined_feature=False).preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_list = {\n",
    "    \"EURUSD\" : EURUSD_train,\n",
    "    \"GBPUSD\" : GBPUSD_train\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dimension = len(dfs_list)\n",
    "state_space = 1 + 3*stock_dimension + len(tech_indicator_list)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 10000, \n",
    "    \"dfs_list\" : dfs_list,\n",
    "    \"initial_amount\": 100000, \n",
    "    \"buy_cost_pct\": 0, \n",
    "    \"sell_cost_pct\": 0, \n",
    "    \"state_space\": state_space, \n",
    "    \"tech_indicator_list\": tech_indicator_list, \n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "e_train_gym = StockTradingEnv(**env_kwargs)\n",
    "print(e_train_gym.data)\n",
    "\n",
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "\n",
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "num_episodes = 5\n",
    "total_timesteps = num_episodes * len(EURUSD_train)\n",
    "\n",
    "A2C_PARAMS = {\"n_steps\": 20, \"ent_coef\": 0.001, \"learning_rate\": 0.002, 'rms_prop_eps': 1e-05}\n",
    "model_a2c = agent.get_model(model_name=\"a2c\",model_kwargs = A2C_PARAMS)\n",
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                            tb_log_name='a2c',\n",
    "                            total_timesteps=total_timesteps)\n",
    "\n",
    "sb3_episode_rewards = trained_a2c.env.envs[0].get_episode_rewards()\n",
    "finrl_episode_rewards = e_train_gym.episode_rewards\n",
    "fig, axs = plt.subplots()\n",
    "\n",
    "axs.plot(finrl_episode_rewards)\n",
    "axs.title.set_text(\"Episode rewards (Gains in NOP) against episodes\")\n",
    "fig.tight_layout()\n",
    "\n",
    "env_test, _ = e_train_gym.get_sb_env()  \n",
    "\n",
    "print(evaluate_policy(trained_a2c, env_test, n_eval_episodes=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EURUSD_trade_df=data.load_ohlc_dataset(\"15min/EURUSD/02_17.csv\")\n",
    "GBPUSD_trade_df=data.load_ohlc_dataset(\"15min/GBPUSD/02_17.csv\")\n",
    "EURUSD_trade, tech_indicator_list = preprocessors.FeatureEngineer(EURUSD_trade_df,\n",
    "                          tech_indicator_params_map = param_map,\n",
    "                          use_technical_indicator=True,\n",
    "                          user_defined_feature=False).preprocess_data()\n",
    "\n",
    "GBPUSD_trade, tech_indicator_list = preprocessors.FeatureEngineer(GBPUSD_trade_df,\n",
    "                          tech_indicator_params_map = param_map,\n",
    "                          use_technical_indicator=True,\n",
    "                          user_defined_feature=False).preprocess_data()\n",
    "dfs_list = {\n",
    "    \"EURUSD\" : EURUSD_trade,\n",
    "    \"GBPUSD\" : GBPUSD_trade\n",
    "}\n",
    "stock_dimension = len(dfs_list)\n",
    "state_space = 1 + 3*stock_dimension + len(tech_indicator_list)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 10000, \n",
    "    \"dfs_list\" : dfs_list,\n",
    "    \"initial_amount\": 100000, \n",
    "    \"buy_cost_pct\": 0, \n",
    "    \"sell_cost_pct\": 0, \n",
    "    \"state_space\": state_space, \n",
    "    \"tech_indicator_list\": tech_indicator_list, \n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_trade_gym = StockTradingEnv(**env_kwargs)\n",
    "env_trade, _ = e_trade_gym.get_sb_env()\n",
    "dfs_list = {\n",
    "    \"EURUSD\" : EURUSD_trade,\n",
    "    \"GBPUSD\" : GBPUSD_trade\n",
    "}\n",
    "stock_dimension = len(dfs_list)\n",
    "state_space = 1 + 3*stock_dimension + len(tech_indicator_list)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "agent = DRLAgent(env = env_trade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateCallbackInstance:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.rewards_arrays = []\n",
    "        self.reward_array_run = []\n",
    "        self.result = None\n",
    "        self.ccy_dims = 0\n",
    "        self.local = None\n",
    "        self.count = 0\n",
    "\n",
    "    def evaluateCallback(self, locals_, globals_):\n",
    "\n",
    "        if self.count == 0 :\n",
    "            self.local = np.copy(locals_[\"obs\"])\n",
    "            self.count += 1\n",
    "\n",
    "        self.reward_array_run.append(locals_[\"reward\"][0])\n",
    "        \n",
    "        if locals_[\"done\"]:\n",
    "            self.rewards_arrays.append(self.reward_array_run)\n",
    "            self.reward_array_run = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_trade_gym.df[\"EURUSD\"].iloc[:, 0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "from typing import List, Tuple\n",
    "\n",
    "def evaluate_policy_rewards(\n",
    "    model: \"base_class.BaseAlgorithm\",\n",
    "    env: VecEnv,\n",
    "    n_eval_episodes: int = 10,\n",
    "    deterministic: bool = True,\n",
    "    render: bool = False\n",
    ") -> Tuple[List[float], List[int], List[int]]:\n",
    "\n",
    "    if isinstance(env, VecEnv):\n",
    "            assert env.num_envs == 1, \"You must pass only one environment when using this function\"\n",
    "    episode_rewards, episode_lengths, rewards_memory_episodes= [], [], []\n",
    "    for i in range(n_eval_episodes):\n",
    "\n",
    "        if not isinstance(env, VecEnv) or i == 0:\n",
    "            obs = env.reset()\n",
    "        rewards_memory = []\n",
    "        done, state = False, None\n",
    "        episode_reward = 0.0\n",
    "        episode_length = 0\n",
    "        while not done:\n",
    "            action, state = model.predict(obs, state=state, deterministic=deterministic)\n",
    "            obs, reward, done, _info = env.step(action)\n",
    "            rewards_memory.append(reward)\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            if render:\n",
    "                env.render()\n",
    "        rewards_memory_episodes.append(rewards_memory)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(episode_length)\n",
    "\n",
    "    return episode_rewards, episode_lengths, rewards_memory_episodes\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy_rewards(trained_a2c, env_trade, deterministic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_trade_gym.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "instance = EvaluateCallbackInstance()\n",
    "mean_episode_rewards, std_episode_rewards = evaluate_policy(model = trained_a2c, env = env_trade, n_eval_episodes=1, callback=instance.evaluateCallback) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import accumulate\n",
    "\n",
    "asd = list(accumulate(instance.rewards_arrays[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance.rewards_arrays[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance.rewards_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locals().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}